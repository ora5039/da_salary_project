# Step 1: Data Collection
# I'll use the glassdoor and linkedin libraries to scrape job postings from Glassdoor and LinkedIn. 

import glassdoor
import linkedin

# Glassdoor API credentials
gd_username = "your_username"
gd_password = "your_password"

# LinkedIn API credentials
li_username = "your_username"
li_password = "your_password"

# Scrape job postings from Glassdoor
gd = glassdoor.Glassdoor(gd_username, gd_password)
job_postings_gd = gd.search_jobs("data scientist", "USA", 100)

# Scrape job postings from LinkedIn
li = linkedin.LinkedIn(li_username, li_password)
job_postings_li = li.search_jobs("data scientist", "USA", 100)

# Combine job postings from both sources
job_postings = job_postings_gd + job_postings_li

# Step 2: Data Preprocessing
# I'll use the pandas library to clean and preprocess the data.

import pandas as pd

# Create a Pandas dataframe from the job postings
df = pd.DataFrame(job_postings)

# Handle missing values
df.fillna("", inplace=True)

# Remove duplicates
df.drop_duplicates(inplace=True)

# Extract relevant information from job descriptions
import nltk
from nltk.tokenize import word_tokenize

nltk.download("punkt")

def extract_skills(job_description):
    tokens = word_tokenize(job_description)
    skills = [token for token in tokens if token.isalpha() and len(token) > 3]
    return skills

df["skills"] = df["job_description"].apply(extract_skills)

# Step 3: Feature Engineering
# I'll extract relevant features from the job postings.

# Step 4: Model Training
# I'll use the sklearn library to train a linear regression model to predict salaries.

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Split the data into training and testing sets
X = df.drop(["salary"], axis=1)
y = df["salary"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Step 5: Model Evaluation
# I'll evaluate the performance of the model using mean absolute error (MAE) and R-squared.

from sklearn.metrics import mean_absolute_error, r2_score

# Evaluate the model on the testing set
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error: {mae:.2f}")
print(f"R-squared: {r2:.2f}")

# Step 6: Model Deployment
# I'll create a simple web application using Flask to deploy the model.

from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route("/predict", methods=["POST"])
def predict():
    job_posting = request.get_json()
    X_new = pd.DataFrame([job_posting])
    y_pred = model.predict(X_new)
    return jsonify({"salary": y_pred[0]
